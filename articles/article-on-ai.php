<?php $thisPage="AI-Article"; ?>
<?php $title="Artificial Intellice: Can Implies Ought"; ?>
<?php include("../topsubfolder.php"); ?>
	<div class="center">
		<h3>Artificial Intelligence: Heading Towards &quotCan Implies Ought&quot?</h3>
	</div>
	<p>One of the many ethical conversations endlessly debated in philosophy is the maxim <i>ought implies can</i>.  This formula, originally from Immanuel Kant, states that if an agent is obligated to perform an action, they must be logically able to perform that action.  That is, there cannot be a logical impediment that bars them from possibly performing that action.  I do not have an obligation to lessen the loss of life in the Bangladesh Liberation War because I did not exist in 1971.  At this stage, it is impossible for me to travel back in time to help with the human-rights crisis that arose from the conflict.  I think about ethical dilemmas all the time, and increasingly, the discussion found in technological circles strays to ethics as our capabilities to engineer our technology to become smarter and more powerful.  Computers and our robots gain new abilities with each new advance in technology, and many of humanity’s brightest minds have started to weigh in with concerns.  Elon Musk, for instance, has described advanced artificial intelligence as, <a href="https://twitter.com/elonmusk/status/495759307346952192">&quotPotentially more dangerous than nukes&quot</a>.  Stephen Hawking has gone so far as to say, <a href="http://www.bbc.com/news/technology-30290540">&quotThe development of full artificial intelligence could spell the end of the human race&quot</a>.  The general worry is that we are not being careful enough, that our advances are not being questioned, and that we aren’t doing enough to safeguard humanity from the dangerous killer AIs from the future.  I believe that the worries of these great thinkers are missing a crucial element of the definition of intelligence, and that true intelligence that we are seeking to design incorporates a more comprehensive understanding of the actions that the agent will perform. There is a missing link between their conception of an AI’s ability to do something and its ability to make the decision of whether or not one ought perform that action.  In other words, Musk, Hawking, and others quickly jump to the worry that for an AI, can implies ought. </p>
	<p>	A recent article in <a href="http://observer.com/2015/08/stephen-hawking-elon-musk-and-bill-gates-warn-about-artificial-intelligence/"><i>The Observer</i></a> spells out the worry that is being repeated from technology’s highest echelons: we have a lot to fear in the future.  Many see Hawking and Musk as being in tune with the pulse of our progress towards strong, true artificial intelligence: &quotThe threats enumerated by Hawking, Musk, and Gates are real and worthy of our immediate attention, despite the immense benefits artificial intelligence can potentially bring to humanity. As robot technology increases steadily toward the advancements necessary to facilitate widespread implementation, it is becoming clear that robots are going to be in situations that pose a number of courses of action. The ethical dilemma of bestowing moral responsibilities on robots calls for rigorous safety and preventative measures that are fail-safe, or the threats are too significant to risk.&quot  It’s undeniable that robots are being designed to fill an increasing amount of roles in society.  Already they are taking over the manufacturing sector, with many companies replacing human workers with automated or robotic processing.  This has done wonders for production and output, while eliminating mistakes and overhead.  Computers are also being designed to out-compete humans in what has been traditionally thought to be areas where only a human-like intelligence might succeed.  We had Deep Blue outclass Garry Kasparov in 1996.  IBM’s Watson mopped the floor with <i>Jeopardy!</i>’s most successful contestants Ken Jennings and Brad Rutter.  Most recently, Google’s AlphaGo defeated one of the best Go players in the world, Lee Sedol, a feat that astonished many.  Go was traditionally seen as too complex to compute, too varied for a computer to calculate in such a way as to outclass a human player.  With this defeat, many saw the future of artificial intelligence as limitless.  Our very conception of the limits of what artificial intelligence could do were shown to be far short of reality.</p>
	<p>The notion of strong artificial intelligence has been around for centuries, and many portrayals of what might the world with AI might look like have graced the page and screen.  We’ve watched HAL9000 dispatch astronauts it saw as dangerous to the success of its mission and the Terminator ruthlessly dispatch anyone foolish enough to get in the way of its mission to kill John Connor.  But, as Musk and other see it, there is a real, genuine threat that might be looming on the horizon.  Computers are surpassing the abilities of humans in more and more complex ways each day.  We have to be careful, we are warned, about the advances we seek to bring about in artificial intelligence.  We are risking our future if we are not careful with what we are doing.  We can design robots to do so many things, and conceivably, we are heading toward a future where we can design an AI to do <i>anything</i>.  While at this point, performing functions on an assembly line, while complex, does not involve ethical considerations that have heavy implications for humankind.  But if we are bringing AI to more and more areas, eventually we will reach the point where AI performs actions that do have ethical import.  And there is a lot of concern for the fact that an advanced AI might decide to perform an action that has disastrous effects on humanity.  Historically, the argue goes, we have not worried about this future, but it is of extreme importance to others that we should begin to worry.</p>
	<p>It is here that I want to raise an objection.  I believe that the conversation of what constitutes strong artificial intelligence is missing a large component.  Generally speaking, in philosophy, a moral agent has to have the ability to think about their actions and be able to understand their intent, consequences, what it matters for others, etc. (this all depends on which ethical theory one bases their ethical framework in!).  When we conceive of what it means to be moral, we are generally speaking about being responsible for one’s actions.  If an agent cannot conceive of certain aspects of their action, for instance, who might be affected by that action, philosophers generally are hesitant to consider that agent morally responsible for that action. I believe that for a human agent to be considered intelligent, they would at least be able to operate with this level of understanding.  I find it difficult to see how a person could be considered intelligent if they could not comprehend the moral component of their actions.  That does not mean in every instance that an agent must act in accord with their moral judgment.  People are not perfect, and sometimes act in their own self-interest or in the interest of other concerns that is in contradiction with what is right.  Nor does it mean that their moral judgment must be unerring.  We are not omniscient, and can get things wrong, including things that affect our moral judgment.  But, generally speaking, being intelligent in a meaningful way must include a moral component.  The definition of intelligence that Musk and Hawking are using here seems to be missing that moral component.</p>
	<p>A morally intelligent agent, even if it is designed, should be able to consider its actions and their moral component.  A truly intelligent being will be able to <b>reflect</b> both before and after on their actions in order to figure out what would be best from many different standpoints, including the one that takes into account whether or not that action is the right thing to do.  So, if that is the case, would this intelligent creation really be so dangerous that it would rival the threat of a nuclear weapon?  If its moral sense is so strong, it would be more likely that humanity has little to fear.  Their moral judgment should be able to prevent the doomsday situations that Hawking is afraid of.  Just because an artificial intelligent system might have the ability to wreak great harm upon humanity, if it is truly intelligent, it would be antithetical to their intelligence to actually enact that great harm.</p>
	<p>In the future, if we are really designing this strong sense of intelligence that mimics or surpasses our own intelligence, we would have to include a moral component in the construction of artificial intelligence.  If we are really designing a strong version of artificial intelligence, we would have to be designing intelligence with a moral component, or else it wouldn’t be strong artificial intelligence at all.  That form of AI that these fearmongers are telling us to fear doesn’t seem to include the moral component of intelligence.  So, really, what are they telling us to fear?  The form of artificial intelligence that they speak of, if that is the future of AI, really isn’t a strong version of artificial intelligence at all.  It seems much more close to a deficient form of intelligence, a dumb brute that does not reflect on what they do or how it might affect others.  So, we are left with two possible lessons from fearmongering: either our future will be full of dangerous but dumb artificial systems, or we will have a future of true artificial intelligence, and little to fear for in terms of our safety.  If the endgame of our artificial intelligence path is the Terminator, not only will we have a lot to fear, but we have also not reached the level of real, strong artificial intelligence.</p>
	</div> 
	</body>
</html>